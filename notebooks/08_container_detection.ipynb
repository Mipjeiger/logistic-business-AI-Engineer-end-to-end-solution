{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab2cf6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import hashlib\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e50ea6",
   "metadata": {},
   "source": [
    "# Set directory //  Convert JSON ‚Üí YOLO TXT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80e6a21",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      2\u001b[39m CLASS_MAP = {\n\u001b[32m      3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcontainer\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m0\u001b[39m,\n\u001b[32m      4\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtruck\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m1\u001b[39m\n\u001b[32m      5\u001b[39m }\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Set up directories\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m BASE_DIR = os.path.dirname(os.path.abspath(\u001b[34;43m__file__\u001b[39;49m))\n\u001b[32m      9\u001b[39m SPLITS = [\u001b[33m\"\u001b[39m\u001b[33mtrain_container\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mvalidation_container\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     10\u001b[39m OUTPUT_DIR = os.path.join(BASE_DIR, \u001b[33m\"\u001b[39m\u001b[33mdatasets_container\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33myolo_dataset\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "# Set class map\n",
    "CLASS_MAP = {\n",
    "    \"container\": 0,\n",
    "    \"truck\": 1\n",
    "}\n",
    "\n",
    "# Set up directories\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "SPLITS = [\"train_container\", \"validation_container\"]\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, \"datasets_container\", \"yolo_dataset\")\n",
    "\n",
    "# Create output directories\n",
    "for split in SPLITS:\n",
    "    split_output_dir = os.path.join(OUTPUT_DIR, split)\n",
    "    os.makedirs(os.path.join(split_output_dir, \"images\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(split_output_dir, \"labels\"), exist_ok=True)\n",
    "\n",
    "\n",
    "def get_file_hash(filepath):\n",
    "    \"\"\"\n",
    "    Calculate MD5 hash of file to detect true duplicates\n",
    "    \"\"\"\n",
    "    hash_md5 = hashlib.md5()\n",
    "    try:\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "                hash_md5.update(chunk)\n",
    "        return hash_md5.hexdigest()\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "def convert_json_to_yolo(json_path, img_path, label_output):\n",
    "    \"\"\"\n",
    "    Convert JSON to YOLO format - supports both bbox and centre/size formats\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(json_path) as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            print(f\"‚ùå Cannot read image: {img_path}\")\n",
    "            return False\n",
    "            \n",
    "        h, w, _ = img.shape\n",
    "        yolo_lines = []\n",
    "        \n",
    "        for obj in data.get(\"labels\", []):\n",
    "            label = obj.get(\"label_class\")\n",
    "            \n",
    "            if label not in CLASS_MAP:\n",
    "                continue\n",
    "            \n",
    "            cls_id = CLASS_MAP[label]\n",
    "            \n",
    "            # Support both formats\n",
    "            if \"centre\" in obj and \"size\" in obj:\n",
    "                cx = obj[\"centre\"][\"x\"] / w\n",
    "                cy = obj[\"centre\"][\"y\"] / h\n",
    "                bw = obj[\"size\"][\"x\"] / w\n",
    "                bh = obj[\"size\"][\"y\"] / h\n",
    "            elif \"bbox\" in obj:\n",
    "                bbox = obj[\"bbox\"]\n",
    "                cx = (bbox[0] + bbox[2] / 2) / w\n",
    "                cy = (bbox[1] + bbox[3] / 2) / h\n",
    "                bw = bbox[2] / w\n",
    "                bh = bbox[3] / h\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            # Validate coordinates\n",
    "            if not (0 <= cx <= 1 and 0 <= cy <= 1 and 0 < bw <= 1 and 0 < bh <= 1):\n",
    "                print(f\"‚ö†Ô∏è  Invalid coordinates in {json_path}\")\n",
    "                continue\n",
    "                \n",
    "            yolo_lines.append(f\"{cls_id} {cx:.6f} {cy:.6f} {bw:.6f} {bh:.6f}\")\n",
    "        \n",
    "        # Write YOLO format\n",
    "        with open(label_output, \"w\") as f:\n",
    "            f.write(\"\\n\".join(yolo_lines))\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error converting {json_path}: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# DUPLICATE DETECTION & PREVENTION\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üîç DUPLICATE DETECTION & CONVERSION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "stats = {\n",
    "    'total': 0,\n",
    "    'success': 0,\n",
    "    'skipped_exists': 0,\n",
    "    'skipped_duplicate': 0,\n",
    "    'failed': 0\n",
    "}\n",
    "\n",
    "# Track processed images by hash to detect true duplicates\n",
    "processed_hashes = defaultdict(list)  # hash -> [list of filenames]\n",
    "processed_filenames = set()  # Track filenames to detect name collisions\n",
    "\n",
    "for split in SPLITS:\n",
    "    print(f\"\\nüìÇ Processing split: {split}\")\n",
    "    \n",
    "    input_dir = os.path.join(BASE_DIR, split)\n",
    "    \n",
    "    if not os.path.exists(input_dir):\n",
    "        print(f\"‚ö†Ô∏è  Input directory not found: {input_dir}\")\n",
    "        continue\n",
    "    \n",
    "    json_files = [f for f in os.listdir(input_dir) if f.endswith(\".json\")]\n",
    "    print(f\"üìä Found {len(json_files)} JSON files\")\n",
    "    \n",
    "    for file in tqdm(json_files, desc=f\"Converting {split}\"):\n",
    "        stats['total'] += 1\n",
    "        \n",
    "        try:\n",
    "            json_path = os.path.join(input_dir, file)\n",
    "            \n",
    "            with open(json_path) as f:\n",
    "                meta = json.load(f)\n",
    "            \n",
    "            img_name = meta.get(\"image_filename\")\n",
    "            if not img_name:\n",
    "                print(f\"‚ùå No 'image_filename' in {file}\")\n",
    "                stats['failed'] += 1\n",
    "                continue\n",
    "            \n",
    "            img_path = os.path.join(input_dir, img_name)\n",
    "            out_img = os.path.join(OUTPUT_DIR, split, \"images\", img_name)\n",
    "            label_name = Path(img_name).stem + \".txt\"\n",
    "            out_lbl = os.path.join(OUTPUT_DIR, split, \"labels\", label_name)\n",
    "            \n",
    "            # ============================================\n",
    "            # CHECK 1: Already processed (skip if exists)\n",
    "            # ============================================\n",
    "            if os.path.exists(out_lbl):\n",
    "                stats['skipped_exists'] += 1\n",
    "                continue\n",
    "            \n",
    "            # ============================================\n",
    "            # CHECK 2: Filename collision detection\n",
    "            # ============================================\n",
    "            if img_name in processed_filenames:\n",
    "                print(f\"‚ö†Ô∏è  FILENAME COLLISION: {img_name} already processed\")\n",
    "                stats['skipped_duplicate'] += 1\n",
    "                continue\n",
    "            \n",
    "            # ============================================\n",
    "            # CHECK 3: True duplicate detection (by hash)\n",
    "            # ============================================\n",
    "            if os.path.exists(img_path):\n",
    "                img_hash = get_file_hash(img_path)\n",
    "                \n",
    "                if img_hash and img_hash in processed_hashes:\n",
    "                    original_file = processed_hashes[img_hash][0]\n",
    "                    print(f\"üîÑ DUPLICATE IMAGE DETECTED:\")\n",
    "                    print(f\"   Original: {original_file}\")\n",
    "                    print(f\"   Duplicate: {img_name}\")\n",
    "                    stats['skipped_duplicate'] += 1\n",
    "                    continue\n",
    "                \n",
    "                # Record this image\n",
    "                processed_hashes[img_hash].append(img_name)\n",
    "                processed_filenames.add(img_name)\n",
    "            else:\n",
    "                print(f\"‚ùå Image not found: {img_path}\")\n",
    "                stats['failed'] += 1\n",
    "                continue\n",
    "            \n",
    "            # ============================================\n",
    "            # COPY & CONVERT (no duplicates detected)\n",
    "            # ============================================\n",
    "            import shutil\n",
    "            shutil.copy2(img_path, out_img)\n",
    "            \n",
    "            if convert_json_to_yolo(json_path, img_path, out_lbl):\n",
    "                stats['success'] += 1\n",
    "            else:\n",
    "                stats['failed'] += 1\n",
    "                # Remove copied image if conversion failed\n",
    "                if os.path.exists(out_img):\n",
    "                    os.remove(out_img)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {file}: {e}\")\n",
    "            stats['failed'] += 1\n",
    "\n",
    "# ========================================\n",
    "# SUMMARY & DUPLICATE REPORT\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä CONVERSION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total files:              {stats['total']}\")\n",
    "print(f\"‚úÖ Success:               {stats['success']}\")\n",
    "print(f\"‚è≠Ô∏è  Skipped (exists):      {stats['skipped_exists']}\")\n",
    "print(f\"üîÑ Skipped (duplicates):  {stats['skipped_duplicate']}\")\n",
    "print(f\"‚ùå Failed:                {stats['failed']}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Report duplicate groups\n",
    "duplicate_groups = {k: v for k, v in processed_hashes.items() if len(v) > 1}\n",
    "if duplicate_groups:\n",
    "    print(f\"\\n‚ö†Ô∏è  Found {len(duplicate_groups)} groups of duplicate images:\")\n",
    "    for hash_val, files in list(duplicate_groups.items())[:5]:  # Show first 5\n",
    "        print(f\"   - {files}\")\n",
    "    if len(duplicate_groups) > 5:\n",
    "        print(f\"   ... and {len(duplicate_groups) - 5} more groups\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ No duplicate images found!\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d05f53",
   "metadata": {},
   "source": [
    "# Handle NEGATIVE Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3156856",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Don't make label txt for negative images\"\"\"\n",
    "NEGATIVE_DIR = os.path.join(BASE_DIR, \"datasets_container\", \"negative\")\n",
    "\n",
    "for file in os.listdir(NEGATIVE_DIR):\n",
    "    if file.endswith('.jpg'):\n",
    "\n",
    "        src_path = os.path.join(NEGATIVE_DIR, file)\n",
    "        dest_path = os.path.join(OUTPUT_DIR, \"train\", \"images\", file)\n",
    "\n",
    "        os.system(f\"cp {src_path} {dest_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ead0920",
   "metadata": {},
   "source": [
    "# Create dataset.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8b9b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_yaml = f\"\"\"\n",
    "path: {OUTPUT_DIR}\n",
    "\n",
    "train: train/images\n",
    "val: validation/images\n",
    "\n",
    "nc: 2\n",
    "\n",
    "names:\n",
    "    0: container\n",
    "    1: truck\n",
    "\"\"\"\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIR, \"dataset.yaml\"), \"w\") as f:\n",
    "    f.write(dataset_yaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842c99b9",
   "metadata": {},
   "source": [
    "# Train YOLOv8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3d9972",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"yolov8n.pt\") # nano = faster model inference\n",
    "\n",
    "results = model.train(\n",
    "    data=os.path.join(OUTPUT_DIR, \"dataset.yaml\"),\n",
    "    epochs=100,\n",
    "    imgsz=640,\n",
    "    batch=16,\n",
    "    device=0, # Use GPU 0\n",
    "    patience=15, # Early stopping if no improvement for 15 epochs\n",
    "    workers=4, # Number of data loading workers\n",
    "    project=os.path.join(BASE_DIR, \"runs_container\"), \n",
    "    name=\"yolov8n_container_detection\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623771d0",
   "metadata": {},
   "source": [
    "# Export Best Model ‚Üí models/container_yolov8.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b26026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export best model to models/container_yolov8.pt\n",
    "\n",
    "import shutil\n",
    "\n",
    "best_model = os.path.join(BASE_DIR, \"runs_container\", \"yolov8n_container_detection\", \"weights\", \"best.pt\")\n",
    "TARGET_PATH = os.path.join(BASE_DIR, \"models\", \"container_yolov8.pt\")\n",
    "\n",
    "# Copy best model to target path\n",
    "shutil.copy(best_model, TARGET_PATH)\n",
    "print(f\"YOLOV8 best model has been exported to {TARGET_PATH} ‚úÖ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c07a65",
   "metadata": {},
   "source": [
    "# Quick Validation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12152eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(TARGET_PATH)\n",
    "img = \"datasets_container/validation/3de22b94583ee0f7defd6c5a6ce439dc2610efad.jpg\" # Example image path\n",
    "\n",
    "# Perform inference\n",
    "results = model.predict(source=img, conf=0.4, save=True)\n",
    "results[0].plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
